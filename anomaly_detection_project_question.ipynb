{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32803e6e6021564d",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='https://sharif.edu/~izadi/images/logo_sharif.png' alt=\"SUT logo\" width=260 height=300 class=\"saturate\">\n",
    "</center>\n",
    "\n",
    "<font face=\"Times New Roman\">\n",
    "    <div dir=ltr align=center>\n",
    "        <font color=0F5298 size=7>\n",
    "            Machine Learning Course Project\n",
    "        </font>\n",
    "        <br><br>\n",
    "        <font color=2565AE size=5>\n",
    "            Computer Engineering Department<br>Lecturer: Dr. Sharifi-Zarchi<br>Spring 2025\n",
    "        </font>\n",
    "        <br><br>\n",
    "        <font color=3C99D size=5>\n",
    "            Project 8: MVTec AD Anomaly Detection\n",
    "        </font>\n",
    "        <br><br>\n",
    "        <font color=6EACDA size=4>\n",
    "            Author: Amir Mohammad Fakhimi\n",
    "        </font>\n",
    "    </div>\n",
    "    <br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f8a66f4aaa03",
   "metadata": {},
   "source": [
    "This notebook is a **course project template** for **MVTec AD anomaly detection**.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Run a **PaDiM baseline** (feature-statistics anomaly detection).\n",
    "2. Implement **your own method** (student method) using **only `train/good`** for training.\n",
    "3. Report **quantitative metrics** (per-category + overall) and **qualitative visualizations**.\n",
    "4. Compare **PaDiM vs your method** on the *same* categories and test images.\n",
    "\n",
    "> This notebook is intentionally \"report-first\": after each model runs, you call a small set of functions to get tables and plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6d55ba2576a0c",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "### What you need\n",
    "- Python + PyTorch + torchvision\n",
    "- Common scientific stack (NumPy / pandas / matplotlib / scikit-learn)\n",
    "- MVTec AD dataset extracted under `DATASET_ROOT`\n",
    "\n",
    "### What you will get\n",
    "- Per-category results (tables + bar plots)\n",
    "- Overall results (macro mean across categories + micro over all test images)\n",
    "- Qualitative overlays (input, anomaly map, overlay + GT contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1871ffeb24514ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:17.936545Z",
     "start_time": "2025-12-21T11:59:16.213195Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, resnet34\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f324e056030b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:17.957377Z",
     "start_time": "2025-12-21T11:59:17.939826Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = 'mps' if torch.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = (DEVICE == 'cuda')\n",
    "\n",
    "print('DEVICE:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ebe7a061c4abb",
   "metadata": {},
   "source": [
    "## 1) Download the dataset (MVTec AD)\n",
    "\n",
    "This project uses the **MVTec Anomaly Detection (MVTec AD)** dataset.\n",
    "\n",
    "1. Download the **whole dataset** (~4.9 GB) from the official downloads page:\n",
    "   - https://www.mvtec.com/company/research/datasets/mvtec-ad/downloads\n",
    "\n",
    "2. Extract it so you end up with a folder named `mvtec_anomaly_detection/` that contains the category folders:\n",
    "   - `bottle/`, `cable/`, `capsule/`, ..., `zipper/`\n",
    "\n",
    "Example (Linux/macOS):\n",
    "\n",
    "```bash\n",
    "# Put the downloaded archive in the same folder as this notebook\n",
    "tar -xf mvtec_anomaly_detection.tar.xz\n",
    "```\n",
    "\n",
    "If you prefer a direct download link, the official site links to a `mydrive.ch` file under \"Download the whole dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5648f25769ec21a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:17.967360Z",
     "start_time": "2025-12-21T11:59:17.965802Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('./mvtec_anomaly_detection')\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666b5f6cc0e28de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:17.980956Z",
     "start_time": "2025-12-21T11:59:17.979288Z"
    }
   },
   "outputs": [],
   "source": [
    "print('DATA_ROOT:', DATASET_ROOT.resolve())\n",
    "print('Exists?', DATASET_ROOT.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404bc142fc0d32c",
   "metadata": {},
   "source": [
    "## 2) Dataset utilities\n",
    "\n",
    "This section provides small helper functions to:\n",
    "- list `.png` images in a directory\n",
    "- collect MVTec paths for one category:\n",
    "  - `train_good`\n",
    "  - `test_good`\n",
    "  - `test_anomaly`\n",
    "  - a mapping from **test-relative key** → **GT mask path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5789bac98a284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:17.987560Z",
     "start_time": "2025-12-21T11:59:17.985132Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_images(directory_path: Path) -> List[Path]:\n",
    "    \"\"\"Return a sorted list of `.png` files inside `directory_path`.\"\"\"\n",
    "    if not directory_path.exists():\n",
    "        return []\n",
    "\n",
    "    return sorted([p for p in directory_path.iterdir() if p.is_file() and p.suffix.lower() == '.png'])\n",
    "\n",
    "\n",
    "def mvtec_collect_category(category_directory: Path) -> Dict[str, object]:\n",
    "    \"\"\"Collect MVTec paths for one category.\n",
    "\n",
    "    Returns a dict with:\n",
    "      - train_good: List[Path]\n",
    "      - test_good: List[Path]\n",
    "      - test_anomaly: List[Path]\n",
    "      - gt_masks_by_name: Dict[str, Path]\n",
    "\n",
    "    The `gt_masks_by_name` key format is:\n",
    "      '<defect_type>/<file_name>.png'  (relative to `test/`)\n",
    "\n",
    "    This avoids filename collisions across defect types (e.g., `scratch/000.png` vs `contamination/000.png`).\n",
    "    \"\"\"\n",
    "    train_good = list_images(category_directory / 'train' / 'good')\n",
    "\n",
    "    test_good = list_images(category_directory / 'test' / 'good')\n",
    "    test_anomaly: List[Path] = []\n",
    "    gt_masks_by_name: Dict[str, Path] = {}\n",
    "\n",
    "    test_directory = category_directory / 'test'\n",
    "    gt_directory = category_directory / 'ground_truth'\n",
    "\n",
    "    if test_directory.exists():\n",
    "        for defect in sorted([p for p in test_directory.iterdir() if p.is_dir() and p.name != 'good']):\n",
    "            test_anomaly.extend(list_images(defect))\n",
    "\n",
    "    if gt_directory.exists():\n",
    "        for defect in sorted([p for p in gt_directory.iterdir() if p.is_dir()]):\n",
    "            for mask_path in list_images(defect):\n",
    "                key = f\"{defect.name}/{mask_path.name.replace('_mask', '')}\"\n",
    "                gt_masks_by_name[key] = mask_path\n",
    "\n",
    "    return {\n",
    "        'train_good': train_good,\n",
    "        'test_good': test_good,\n",
    "        'test_anomaly': sorted(test_anomaly),\n",
    "        'gt_masks_by_name': gt_masks_by_name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70347e7deadc0895",
   "metadata": {},
   "source": [
    "## 3) Dataset + DataLoaders (6 points)\n",
    "\n",
    "We wrap MVTec paths into a `Dataset` that returns:\n",
    "\n",
    "- `image` (tensor, resized to 224×224)\n",
    "- `label` (0 = good, 1 = anomaly)\n",
    "- `mask` (binary tensor) if GT exists, otherwise `None`\n",
    "- `image_path` (string)\n",
    "\n",
    "We also define a collate function so the batch keeps `image_path` and `mask` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5c8d0c5362a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.001302Z",
     "start_time": "2025-12-21T11:59:17.999926Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2bb1e1b6e8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.005204Z",
     "start_time": "2025-12-21T11:59:18.003750Z"
    }
   },
   "outputs": [],
   "source": [
    "padim_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10d02d4627d284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.010028Z",
     "start_time": "2025-12-21T11:59:18.007578Z"
    }
   },
   "outputs": [],
   "source": [
    "class MVTecDataset(Dataset):\n",
    "    \"\"\"A minimal dataset wrapper for MVTec AD.\n",
    "\n",
    "    Each item returns a dict:\n",
    "      - image: FloatTensor (3, 224, 224)\n",
    "      - label: LongTensor scalar (0=good, 1=anomaly)\n",
    "      - mask: FloatTensor (1, 224, 224) with {0,1} values, or None (for good images)\n",
    "      - image_path: str\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[Path],\n",
    "        labels: List[int],\n",
    "        mask_paths: Optional[List[Optional[Path]]] = None,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"Create a dataset.\n",
    "\n",
    "        Args:\n",
    "            image_paths: Paths to images.\n",
    "            labels: 0 for good, 1 for anomaly (aligned with `image_paths`).\n",
    "            mask_paths: Optional list of GT mask paths (aligned with `image_paths`).\n",
    "            transform: torchvision transform applied to the PIL image.\n",
    "        \"\"\"\n",
    "        assert len(image_paths) == len(labels), 'image_paths and labels must have the same length'\n",
    "        if mask_paths is not None:\n",
    "            assert len(mask_paths) == len(image_paths), 'mask_paths must align with image_paths'\n",
    "\n",
    "        # TODO (2 points)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of items in the dataset.\"\"\"\n",
    "        # TODO (1 points)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, object]:\n",
    "        \"\"\"Load one image (and optional mask) and return a dict.\"\"\"\n",
    "        # TODO (2 points)\n",
    "\n",
    "        mask = None\n",
    "        # Keep masks aligned with the resized image (224×224).\n",
    "        # TODO (1 points)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'mask': mask,\n",
    "            'image_path': str(image_path),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed5d62bf8e0b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.014222Z",
     "start_time": "2025-12-21T11:59:18.012466Z"
    }
   },
   "outputs": [],
   "source": [
    "def mvtec_collate_fn(batch: List[Dict[str, object]]) -> Dict[str, object]:\n",
    "    \"\"\"Collate function that keeps image paths and (optional) masks in the batch.\"\"\"\n",
    "    images = torch.stack([b['image'] for b in batch], dim=0)\n",
    "    labels = torch.stack([b['label'] for b in batch], dim=0)\n",
    "    paths = [b['image_path'] for b in batch]\n",
    "    masks = [b['mask'] for b in batch]\n",
    "\n",
    "    return {'image': images, 'label': labels, 'image_path': paths, 'mask': masks}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdc6e6ae4539f",
   "metadata": {},
   "source": [
    "## 4) Dataset sanity checks (stats + visuals)\n",
    "\n",
    "Before running models, we:\n",
    "- check basic dataset counts per category\n",
    "- pick **fixed samples** (good + anomaly with GT available)\n",
    "- visualize those samples so qualitative results are consistent across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442f5e24f559976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.017970Z",
     "start_time": "2025-12-21T11:59:18.016567Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_categories(data_root: Path) -> List[str]:\n",
    "    \"\"\"Return category folder names under `data_root` (e.g., 'bottle', 'cable', ...).\"\"\"\n",
    "    if not data_root.exists():\n",
    "        return []\n",
    "\n",
    "    return sorted([p.name for p in data_root.iterdir() if p.is_dir() and not p.name.startswith('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d0c131e04294f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.022380Z",
     "start_time": "2025-12-21T11:59:18.020393Z"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = get_categories(DATASET_ROOT)\n",
    "\n",
    "print('Num categories:', len(CATEGORIES))\n",
    "print('First 5:', CATEGORIES[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34c401c44897c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.029559Z",
     "start_time": "2025-12-21T11:59:18.027075Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_fixed_samples_by_category(\n",
    "    dataset_root: Path,\n",
    "    categories: List[str],\n",
    "    seed: int = 0,\n",
    "    n_good: int = 1,\n",
    "    n_anom: int = 1,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"Select fixed sample image paths per category for qualitative visualization.\n",
    "\n",
    "    We pick:\n",
    "      - `n_good` images from `test/good`\n",
    "      - `n_anom` images from anomaly folders in `test/` **only if a GT mask exists**\n",
    "\n",
    "    Returns:\n",
    "        Dict[category -> List[path_str]] (good samples first, then anomalies).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    out: Dict[str, List[str]] = {}\n",
    "\n",
    "    for category in categories:\n",
    "        data = mvtec_collect_category(dataset_root / category)\n",
    "        good_paths: List[Path] = list(data['test_good'])\n",
    "        anomaly_paths: List[Path] = list(data['test_anomaly'])\n",
    "        gt_by_name: Dict[str, Path] = dict(data['gt_masks_by_name'])\n",
    "\n",
    "        # Keep only anomalies that have GT masks (so contours/metrics are meaningful).\n",
    "        anomaly_paths = [\n",
    "            p for p in anomaly_paths\n",
    "            if str(p.relative_to(dataset_root / category / 'test')).replace('\\\\', '/') in gt_by_name\n",
    "        ]\n",
    "\n",
    "        sel: List[str] = []\n",
    "\n",
    "        if n_good > 0 and len(good_paths) > 0:\n",
    "            g_idx = rng.choice(len(good_paths), size=min(n_good, len(good_paths)), replace=False)\n",
    "            sel.extend([str(good_paths[j]) for j in g_idx])\n",
    "\n",
    "        if n_anom > 0 and len(anomaly_paths) > 0:\n",
    "            a_idx = rng.choice(len(anomaly_paths), size=min(n_anom, len(anomaly_paths)), replace=False)\n",
    "            sel.extend([str(anomaly_paths[j]) for j in a_idx])\n",
    "\n",
    "        out[category] = sel\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23239bbcfb19df2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.097020Z",
     "start_time": "2025-12-21T11:59:18.039691Z"
    }
   },
   "outputs": [],
   "source": [
    "QUAL_SAMPLE_PATHS_BY_CAT = select_fixed_samples_by_category(\n",
    "    DATASET_ROOT, CATEGORIES, seed=SEED, n_good=1, n_anom=1\n",
    ")\n",
    "\n",
    "print('Example qualitative samples (first 3 categories):')\n",
    "for c in CATEGORIES[:3]:\n",
    "    print(' -', c, '->', [Path(p).name for p in QUAL_SAMPLE_PATHS_BY_CAT[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90018117de90fcf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.103002Z",
     "start_time": "2025-12-21T11:59:18.101318Z"
    }
   },
   "outputs": [],
   "source": [
    "def category_stats(category: str) -> Dict[str, object]:\n",
    "    \"\"\"Compute a few quick dataset statistics for one MVTec category.\"\"\"\n",
    "    category_data = mvtec_collect_category(DATASET_ROOT / category)\n",
    "    defect_types = sorted({Path(p).parent.name for p in category_data['test_anomaly']})\n",
    "\n",
    "    return {\n",
    "        'category': category,\n",
    "        'train_good': len(category_data['train_good']),\n",
    "        'test_good': len(category_data['test_good']),\n",
    "        'test_anomaly': len(category_data['test_anomaly']),\n",
    "        'num_defect_types': len(defect_types),\n",
    "        'defect_types': ', '.join(defect_types),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e301f306facd381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.158596Z",
     "start_time": "2025-12-21T11:59:18.114624Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(CATEGORIES):\n",
    "    df_stats = pd.DataFrame([category_stats(c) for c in CATEGORIES])\n",
    "    display(df_stats)\n",
    "else:\n",
    "    print('No categories found. Check DATA_ROOT.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec9b74ed8a0030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.283406Z",
     "start_time": "2025-12-21T11:59:18.186027Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'df_stats' in globals() and len(df_stats):\n",
    "    x = np.arange(len(df_stats))\n",
    "    width = 0.25\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.bar(x - width, df_stats['train_good'].values, width=width, label='train/good')\n",
    "    plt.bar(x, df_stats['test_good'].values, width=width, label='test/good')\n",
    "    plt.bar(x + width, df_stats['test_anomaly'].values, width=width, label='test/anomaly')\n",
    "\n",
    "    plt.xticks(x, df_stats['category'].values, rotation=60, ha='right')\n",
    "    plt.ylabel('Number of images')\n",
    "\n",
    "    plt.title('MVTec AD: image counts per category')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e7d2e3f586af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:18.300293Z",
     "start_time": "2025-12-21T11:59:18.297748Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_fixed_samples(category: str) -> None:\n",
    "    \"\"\"Show the fixed qualitative samples (good + anomaly) picked for a category.\"\"\"\n",
    "    sample_paths = QUAL_SAMPLE_PATHS_BY_CAT.get(category, [])\n",
    "    if not sample_paths:\n",
    "        print('No fixed samples for', category, '(QUAL_SAMPLE_PATHS_BY_CAT empty).')\n",
    "        return\n",
    "\n",
    "    samples = []\n",
    "    for p in sample_paths:\n",
    "        p = str(p)\n",
    "        tag = 'good' if Path(p).parent.name == 'good' else 'anomaly'\n",
    "        samples.append((tag, p))\n",
    "\n",
    "    cols = min(4, len(samples))\n",
    "    rows = int(math.ceil(len(samples) / cols))\n",
    "\n",
    "    plt.figure(figsize=(12, 3.5 * rows))\n",
    "    for i, (tag, p) in enumerate(samples, 1):\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{category} | {tag}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7cbd7f8c594cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.246684Z",
     "start_time": "2025-12-21T11:59:18.303014Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(CATEGORIES):\n",
    "    for category in CATEGORIES:\n",
    "        show_fixed_samples(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfadd50437590fd",
   "metadata": {},
   "source": [
    "## 5) PaDiM baseline implementation (30 points)\n",
    "\n",
    "PaDiM (Patch Distribution Modeling) builds a **Gaussian model** over deep features extracted from a pretrained backbone.\n",
    "\n",
    "High-level steps per category:\n",
    "1. Extract intermediate backbone features on `train/good`\n",
    "2. Build per-location feature distribution (mean + covariance)\n",
    "3. Score each test image by **Mahalanobis distance** to the learned distribution\n",
    "4. Produce:\n",
    "   - pixel-level anomaly map\n",
    "   - image-level score (aggregation of top anomaly pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff86edf2922e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.289390Z",
     "start_time": "2025-12-21T11:59:20.286381Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"Extract intermediate feature maps from a pretrained ResNet backbone.\n",
    "\n",
    "    Args:\n",
    "        backbone: 'resnet18' or 'resnet34'\n",
    "        pretrained: whether to load ImageNet weights\n",
    "        layers: which layer feature maps to return (e.g., ('layer1','layer2','layer3'))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str = 'resnet18',\n",
    "        pretrained: bool = True,\n",
    "        layers: Tuple[str, ...] = ('layer1', 'layer2', 'layer3'),\n",
    "    ):\n",
    "        \"\"\"Initialize the backbone and register which layers to return.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone == 'resnet18':\n",
    "            # TODO (1 points)\n",
    "            pass\n",
    "        elif backbone == 'resnet34':\n",
    "            # TODO (1 points)\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unsupported backbone: ' + backbone)\n",
    "\n",
    "        # TODO (1 points)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"Return a list of feature maps, ordered as `self.layers`.\"\"\"\n",
    "        # TODO (3 points)\n",
    "\n",
    "        return [features[k] for k in self.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9aaf4c8feaeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.294117Z",
     "start_time": "2025-12-21T11:59:20.292569Z"
    }
   },
   "outputs": [],
   "source": [
    "def align_and_concat_features(\n",
    "    feature_list: List[torch.Tensor],\n",
    "    target_hw: Tuple[int, int] = (28, 28),\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Resize feature maps to `target_hw` and concatenate them along channel dimension.\n",
    "\n",
    "    This produces a single embedding tensor of shape (B, C_total, H, W).\n",
    "    \"\"\"\n",
    "    aligned = []\n",
    "    for f in feature_list:\n",
    "        if f.shape[-2:] != target_hw:\n",
    "            f = F.interpolate(f, size=target_hw, mode='bilinear', align_corners=False)\n",
    "        aligned.append(f)\n",
    "    return torch.cat(aligned, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1f02e1c8da63c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.297952Z",
     "start_time": "2025-12-21T11:59:20.296504Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_channel_indices(total_channels: int, d: int, seed: int = 0) -> np.ndarray:\n",
    "    \"\"\"Select `d` channels out of `total_channels` (deterministic given `seed`).\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    if d >= total_channels:\n",
    "        return np.arange(total_channels)\n",
    "    index = rng.choice(total_channels, size=d, replace=False)\n",
    "\n",
    "    return np.sort(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c43db6e6b33d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.302649Z",
     "start_time": "2025-12-21T11:59:20.300588Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(\n",
    "    feature_model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    channel_index: Optional[np.ndarray],\n",
    "    target_hw: Tuple[int, int],\n",
    "    device: str,\n",
    ") -> Tuple[torch.Tensor, List[str], List[int], List[object]]:\n",
    "    \"\"\"Extract aligned embeddings for all images in a loader.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: Tensor (N, C, H, W) on CPU\n",
    "        paths: list of image path strings\n",
    "        labels: list of int labels (0/1)\n",
    "        masks: list of masks (tensor or None), aligned with paths\n",
    "    \"\"\"\n",
    "    feature_model.eval()\n",
    "\n",
    "    # TODO (6 points)\n",
    "    return embeddings, all_paths, all_labels, all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f50a867e60d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.307013Z",
     "start_time": "2025-12-21T11:59:20.305117Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_padim_gaussian(train_embeddings: torch.Tensor, cov_eps: float = 1e-3) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Fit per-location Gaussian parameters for PaDiM.\n",
    "\n",
    "    Args:\n",
    "        train_embeddings: Tensor (N, C, H, W) from train/good images.\n",
    "        cov_eps: Small diagonal jitter for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        mu_map: Tensor (C, H, W) mean feature at each location.\n",
    "        cov_inv: Tensor (H*W, C, C) inverse covariance per location.\n",
    "    \"\"\"\n",
    "    # TODO (8 points)\n",
    "    return mu_map, cov_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d953c4a7de7063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.311278Z",
     "start_time": "2025-12-21T11:59:20.309534Z"
    }
   },
   "outputs": [],
   "source": [
    "def padim_score_maps(test_embeddings: torch.Tensor, mu_map: torch.Tensor, cov_inv: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute PaDiM anomaly score maps (Mahalanobis distance) for test embeddings.\n",
    "\n",
    "    Returns:\n",
    "        score_maps: Tensor (N, H, W) where higher values indicate more anomalous pixels.\n",
    "    \"\"\"\n",
    "    # TODO (6 points)\n",
    "    return scores.reshape(n, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fafb0a316a92c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.315550Z",
     "start_time": "2025-12-21T11:59:20.313678Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize an array to [0, 1] (safe for constant arrays).\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if mx - mn < 1e-12:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def aggregate_image_score(score_map: np.ndarray, aggregation: str = 'mean_topk', topk_frac: float = 0.1) -> float:\n",
    "    \"\"\"Aggregate a pixel anomaly map into a single image-level score.\"\"\"\n",
    "    s = score_map.reshape(-1)\n",
    "\n",
    "    if aggregation == 'max':\n",
    "        return float(s.max())\n",
    "    if aggregation == 'mean':\n",
    "        return float(s.mean())\n",
    "    if aggregation == 'mean_topk':\n",
    "        k = max(1, int(topk_frac * len(s)))\n",
    "        return float(np.mean(np.partition(s, -k)[-k:]))\n",
    "\n",
    "    raise ValueError('Unknown aggregation: ' + aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c16e40735c1408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.320730Z",
     "start_time": "2025-12-21T11:59:20.318040Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PadimConfig:\n",
    "    \"\"\"Configuration for running PaDiM on a single MVTec category.\"\"\"\n",
    "\n",
    "    category: str\n",
    "    backbone: str = 'resnet18'\n",
    "    pretrained: bool = True\n",
    "    layers: Tuple[str, ...] = ('layer1', 'layer2', 'layer3')\n",
    "    target_hw: Tuple[int, int] = (28, 28)\n",
    "\n",
    "    # Random channel subsampling (PaDiM uses a subset of channels for efficiency)\n",
    "    d: int = 100\n",
    "\n",
    "    # Gaussian fitting stability\n",
    "    covariance_epsilon: float = 1e-3\n",
    "\n",
    "    # Optional post-processing on anomaly maps\n",
    "    smooth_sigma: Optional[float] = 2.0  # set None to disable smoothing\n",
    "\n",
    "    # Image score aggregation\n",
    "    image_aggregation: str = 'mean_topk'\n",
    "    topk_fraction: float = 0.1\n",
    "\n",
    "    # DataLoader settings\n",
    "    batch_size: int = 32\n",
    "\n",
    "\n",
    "def gaussian_smooth_np(m: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"Apply a simple separable Gaussian blur to a 2D numpy array.\"\"\"\n",
    "    radius = int(3 * sigma)\n",
    "    if radius <= 0:\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "    xs = np.arange(-radius, radius + 1)\n",
    "    k = np.exp(-(xs ** 2) / (2 * sigma ** 2))\n",
    "    k = (k / k.sum()).astype(np.float32)\n",
    "\n",
    "    # Convolve over height\n",
    "    tmp = np.pad(m, ((radius, radius), (0, 0)), mode='reflect')\n",
    "    out = np.zeros_like(m, dtype=np.float32)\n",
    "    for i in range(m.shape[0]):\n",
    "        out[i] = (tmp[i:i + 2 * radius + 1] * k[:, None]).sum(axis=0)\n",
    "\n",
    "    # Convolve over width\n",
    "    tmp2 = np.pad(out, ((0, 0), (radius, radius)), mode='reflect')\n",
    "    out2 = np.zeros_like(out, dtype=np.float32)\n",
    "    for j in range(out.shape[1]):\n",
    "        out2[:, j] = (tmp2[:, j:j + 2 * radius + 1] * k[None, :]).sum(axis=1)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec05d74e9eb7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.327166Z",
     "start_time": "2025-12-21T11:59:20.323056Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_padim_category(config: PadimConfig) -> Dict[str, object]:\n",
    "    \"\"\"Run PaDiM on one category and return a result dict used by the reporting code.\"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    d = mvtec_collect_category(DATASET_ROOT / config.category)\n",
    "    train_paths = d['train_good']\n",
    "    test_paths = d['test_good'] + d['test_anomaly']\n",
    "    test_labels = [0] * len(d['test_good']) + [1] * len(d['test_anomaly'])\n",
    "\n",
    "    mask_paths = []\n",
    "    for p, y in zip(test_paths, test_labels):\n",
    "        if y == 0:\n",
    "            mask_paths.append(None)\n",
    "        else:\n",
    "            rel_key = str(Path(p).relative_to(DATASET_ROOT / config.category / 'test')).replace('\\\\', '/')\n",
    "            mask_paths.append(d['gt_masks_by_name'].get(rel_key, None))\n",
    "\n",
    "    train_dataset = MVTecDataset(train_paths, labels=[0] * len(train_paths), mask_paths=None, transform=padim_transform)\n",
    "    test_dataset = MVTecDataset(test_paths, labels=test_labels, mask_paths=mask_paths, transform=padim_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, collate_fn=mvtec_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, collate_fn=mvtec_collate_fn)\n",
    "\n",
    "    feature_model = ResNetFeatureExtractor(backbone=config.backbone, pretrained=config.pretrained, layers=config.layers).to(DEVICE)\n",
    "\n",
    "    # TODO (4 points)\n",
    "\n",
    "    scores = [aggregate_image_score(score_maps[i], aggregation=config.image_aggregation, topk_frac=config.topk_fraction)\n",
    "              for i in\n",
    "              range(len(score_maps))]\n",
    "    scores = normalize_01(np.array(scores))\n",
    "\n",
    "    img_m = image_metrics(labels_out, scores.tolist())\n",
    "\n",
    "    pixel_aucs = []\n",
    "    best_dices = []\n",
    "    for i in range(len(labels_out)):\n",
    "        if labels_out[i] == 1 and masks_out[i] is not None:\n",
    "            gt = masks_out[i].numpy().squeeze(0)\n",
    "            pixel_aucs.append(compute_pixel_auc(gt, score_maps[i]))\n",
    "            best_dices.append(compute_best_dice(gt, normalize_01(score_maps[i])))\n",
    "\n",
    "    pix_auc = float(np.nanmean(pixel_aucs)) if len(pixel_aucs) else float('nan')\n",
    "    dice = float(np.nanmean(best_dices)) if len(best_dices) else float('nan')\n",
    "\n",
    "    out = {\n",
    "        'category': config.category,\n",
    "        'image_metrics': img_m,\n",
    "        'pixel_auroc': pix_auc,\n",
    "        'pixel_best_dice': dice,\n",
    "        'paths': paths,\n",
    "        'labels': labels_out,\n",
    "        'scores': scores.tolist(),\n",
    "        'score_maps': score_maps,\n",
    "        'masks': masks_out,\n",
    "        'runtime_sec': float(time.time() - t0),\n",
    "        'config': config,\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c34e3dc8d76dd",
   "metadata": {},
   "source": [
    "## 6) Student implementation (60 points)\n",
    "\n",
    "Implement your own anomaly detection method with the same interface:\n",
    "\n",
    "- `fit(train_loader)` uses **only `train/good`**\n",
    "- `predict(test_loader)` must return:\n",
    "  - `paths`: list of test paths in the same order as the loader\n",
    "  - `scores`: anomaly scores (higher = more anomalous)\n",
    "  - `score_maps`: pixel anomaly maps aligned with `paths` (required for pixel metrics + qualitative overlays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa76cc32ee02e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.331039Z",
     "start_time": "2025-12-21T11:59:20.329526Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO (60 points)\n",
    "#   fit (40 points)\n",
    "#     Core correctness & hygiene (10 points)\n",
    "#       - no leakage, uses only train/good, reproducible, stable (no NaNs)\n",
    "#     Image-level modeling (10 points)\n",
    "#       - learns/calibrates image anomaly scores (e.g., feature stats, density model)\n",
    "#     Pixel-level preparation (20 points)\n",
    "#       - learns/calibrates spatial/local representation needed for score maps\n",
    "#   predict (20 points)\n",
    "#     Image-level output (10 points)\n",
    "#       - correct keys, correct ordering, meaningful non-constant scores\n",
    "#     Pixel-level output (10 points)\n",
    "#       - correct score_maps shape/alignment, meaningful maps, evaluator-safe\n",
    "\n",
    "\n",
    "class StudentMethod:\n",
    "    \"\"\"Interface for the student anomaly detector.\n",
    "\n",
    "    Rules\n",
    "    - Training must use **only `train/good`** images.\n",
    "    - `predict` must return **image-level scores** AND **pixel-level score maps**.\n",
    "\n",
    "    Expected output format from `predict`:\n",
    "      {\n",
    "        'paths': List[str],          # test image paths (same order as the loader)\n",
    "        'scores': List[float],       # length N, higher = more anomalous\n",
    "        'score_maps': np.ndarray,    # shape (N, H, W), higher = more anomalous per pixel\n",
    "        'runtime_sec': float,\n",
    "      }\n",
    "\n",
    "    Notes\n",
    "    - `score_maps[i]` must correspond to `paths[i]`.\n",
    "    - Spatial size (H, W) can be lower than the input image; evaluation will upsample as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, train_loader: DataLoader) -> None:\n",
    "        \"\"\"Fit your method using ONLY `train/good` images.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, test_loader: DataLoader) -> Dict[str, object]:\n",
    "        \"\"\"Predict anomaly scores and pixel anomaly maps for test images.\n",
    "\n",
    "        Returns:\n",
    "            Dict with keys: `paths`, `scores`, `score_maps`, and `runtime_sec`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41854c7de005e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.334701Z",
     "start_time": "2025-12-21T11:59:20.333509Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "student_out_by_cat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9740bcd964078f2",
   "metadata": {},
   "source": [
    "## 7) Metrics + Reporting utilities (10 points)\n",
    "\n",
    "### Image-level metrics\n",
    "- AUROC\n",
    "- Average Precision (AP)\n",
    "- Best F1 (threshold swept on PR curve)\n",
    "- Balanced Accuracy (at best-F1 threshold)\n",
    "\n",
    "### Pixel-level metrics\n",
    "All methods (PaDiM and Student) must provide `score_maps`, so we also compute:\n",
    "- Pixel AUROC\n",
    "- Best Dice (threshold swept)\n",
    "\n",
    "### Reporting\n",
    "We summarize a model output into:\n",
    "- per-category table\n",
    "- overall table (macro mean + micro pooled)\n",
    "\n",
    "Plotting helpers take those tables and draw charts (bars + simple comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a253bf09705dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.403403Z",
     "start_time": "2025-12-21T11:59:20.336959Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_best_f1(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Compute the best F1 score by sweeping the PR-curve thresholds.\n",
    "\n",
    "    Returns:\n",
    "        (best_f1, best_threshold)\n",
    "    \"\"\"\n",
    "    # TODO (2 points)\n",
    "\n",
    "\n",
    "def balanced_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Balanced accuracy = (TPR + TNR) / 2.\"\"\"\n",
    "    # TODO (2 points)\n",
    "\n",
    "\n",
    "def image_metrics(y_true: np.ndarray, y_score: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute image-level anomaly detection metrics from labels and anomaly scores.\n",
    "\n",
    "    This function evaluates a model that outputs *one anomaly score per image*.\n",
    "    Higher scores are assumed to indicate \"more anomalous\".\n",
    "\n",
    "    Args:\n",
    "        y_true:\n",
    "            Array-like of shape (N,). Binary ground-truth labels per image.\n",
    "            Convention in this notebook: 0 = normal/good, 1 = anomalous/defective.\n",
    "        y_score:\n",
    "            Array-like of shape (N,). Real-valued anomaly scores per image.\n",
    "            Larger values should correspond to higher anomaly likelihood.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the following keys (float values):\n",
    "            - 'auroc':\n",
    "                Area Under the ROC Curve computed from (y_true, y_score). Measures\n",
    "                ranking quality across all thresholds. Undefined if only one class\n",
    "                is present in y_true.\n",
    "            - 'ap':\n",
    "                Average Precision (area under the precision-recall curve).\n",
    "                Particularly informative under class imbalance. Undefined if only\n",
    "                one class is present.\n",
    "            - 'best_f1':\n",
    "                Maximum F1 score achieved by thresholding y_score. The threshold\n",
    "                is selected by `compute_best_f1` (typically scanning over candidate\n",
    "                thresholds derived from scores).\n",
    "            - 'best_thr':\n",
    "                The score threshold that achieves 'best_f1'. Prediction rule:\n",
    "                `y_pred = (y_score >= best_thr)`.\n",
    "            - 'bal_acc':\n",
    "                Balanced accuracy at `best_thr`, i.e. average of TPR and TNR.\n",
    "                Useful when the dataset is imbalanced.\n",
    "\n",
    "    Notes:\n",
    "        - If N == 0, or inputs are empty, all metrics return NaN.\n",
    "        - If y_true contains only one class (all 0s or all 1s), AUROC and AP are\n",
    "          mathematically undefined; this function returns NaN for all metrics to\n",
    "          avoid misleading values.\n",
    "        - The chosen 'best_thr' is *computed on the evaluated split*. In a strict\n",
    "          experimental setup you would choose thresholds on a validation set, then\n",
    "          report test metrics using that fixed threshold.\n",
    "    \"\"\"\n",
    "    # TODO (2 points)\n",
    "\n",
    "\n",
    "def compute_pixel_auc(gt_mask: np.ndarray, score_map: np.ndarray) -> float:\n",
    "    \"\"\"Compute pixel-level AUROC for a *single image* using a GT mask and a score map.\n",
    "\n",
    "    This evaluates segmentation-style anomaly detection, where the model outputs an\n",
    "    anomaly score for each pixel. Higher scores are assumed to indicate higher\n",
    "    anomaly likelihood.\n",
    "\n",
    "    Args:\n",
    "        gt_mask:\n",
    "            Binary ground-truth mask of shape (H, W). Values are interpreted as:\n",
    "            0 = normal pixel, 1 = anomalous/defect pixel.\n",
    "        score_map:\n",
    "            Per-pixel anomaly scores. Typically shape (H, W), but can differ if the\n",
    "            model outputs a lower-resolution map. In that case, this function will\n",
    "            upsample the score map to (H, W) using bilinear interpolation.\n",
    "\n",
    "    Returns:\n",
    "        Pixel AUROC as a float. Returns NaN if the ground-truth mask contains only\n",
    "        one class (all zeros or all ones), because AUROC is undefined in that case.\n",
    "\n",
    "    Behavior:\n",
    "        - If `gt_mask.shape != score_map.shape`, `score_map` is resized to\n",
    "          `gt_mask.shape` via torch + bilinear interpolation (align_corners=False).\n",
    "        - The metric is computed by flattening both arrays to 1D vectors and\n",
    "          calling `roc_auc_score(y_true, y_score)`.\n",
    "    \"\"\"\n",
    "    # TODO (2 points)\n",
    "\n",
    "\n",
    "def compute_best_dice(gt_mask: np.ndarray, score_map: np.ndarray) -> float:\n",
    "    \"\"\"Compute the best Dice score over a fixed threshold grid for a single image.\n",
    "\n",
    "    This is a fast approximation of the \"best possible\" Dice you can get by choosing\n",
    "    a threshold to binarize a score map into a predicted anomaly mask.\n",
    "\n",
    "    Args:\n",
    "        gt_mask:\n",
    "            Binary ground-truth mask of shape (H, W). Values are interpreted as:\n",
    "            0 = normal pixel, 1 = anomalous/defect pixel.\n",
    "        score_map:\n",
    "            Per-pixel anomaly scores. Expected shape (H, W), but if different,\n",
    "            the function upsamples `score_map` to (H, W) with bilinear interpolation.\n",
    "\n",
    "    Returns:\n",
    "        The maximum Dice coefficient achieved across thresholds in\n",
    "        `np.linspace(0.0, 1.0, 51)`.\n",
    "\n",
    "    Definition:\n",
    "        For a threshold t, predicted mask is `pred = (score_map >= t)`.\n",
    "        Dice(pred, gt) = 2 * |pred ∩ gt| / (|pred| + |gt|).\n",
    "        If both pred and gt are empty (denominator == 0), Dice is defined as 1.0.\n",
    "    \"\"\"\n",
    "    # TODO (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a50446f7c768a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.410970Z",
     "start_time": "2025-12-21T11:59:20.406319Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_examples(\n",
    "    model_out: Dict[str, object],\n",
    "    *,\n",
    "    gt_out: Optional[Dict[str, object]] = None,\n",
    "    sample_paths: Optional[List[str]] = None,\n",
    "    n: int = 6,\n",
    "    seed: int = 0,\n",
    "    vis_size: Tuple[int, int] = (224, 224),\n",
    "    title_prefix: str = '',\n",
    ") -> None:\n",
    "    \"\"\"Visualize anomaly maps and optional GT contours for selected images.\n",
    "\n",
    "    Works for both PaDiM and the student method.\n",
    "\n",
    "    Per row:\n",
    "      1) Input image\n",
    "      2) Upsampled anomaly map (normalized to [0, 1])\n",
    "      3) Overlay (image + anomaly map) with GT contour (if available)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `model_out` must contain `paths` and `score_maps`.\n",
    "    - If `gt_out` is provided, `labels/masks` are taken from it; otherwise from `model_out`.\n",
    "    \"\"\"\n",
    "    H, W = vis_size\n",
    "\n",
    "    if 'paths' not in model_out:\n",
    "        raise ValueError('model_out is missing required key \"paths\".')\n",
    "    if 'score_maps' not in model_out:\n",
    "        raise ValueError('model_out is missing required key \"score_maps\".')\n",
    "\n",
    "    model_paths: List[str] = [str(p) for p in model_out['paths']]\n",
    "    maps = np.asarray(model_out['score_maps'], dtype=np.float32)\n",
    "    if maps.ndim != 3:\n",
    "        raise ValueError(f'score_maps must have shape (N, H, W); got {maps.shape}.')\n",
    "    if len(model_paths) != maps.shape[0]:\n",
    "        raise ValueError(f'paths length ({len(model_paths)}) != score_maps N ({maps.shape[0]}).')\n",
    "\n",
    "    # For student outputs, we want labels/masks from GT (PaDiM outputs / dataset).\n",
    "    ref = gt_out if gt_out is not None else model_out\n",
    "    ref_paths: List[str] = [str(p) for p in ref.get('paths', model_paths)]\n",
    "    labels: List[int] = list(ref.get('labels', [0] * len(ref_paths)))\n",
    "    masks = ref.get('masks', None)\n",
    "\n",
    "    model_path_to_idx = {p: i for i, p in enumerate(model_paths)}\n",
    "    ref_path_to_idx = {p: i for i, p in enumerate(ref_paths)}\n",
    "\n",
    "    # Choose which paths to display.\n",
    "    if sample_paths is not None and len(sample_paths):\n",
    "        chosen = [str(p) for p in sample_paths]\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        chosen = rng.choice(ref_paths, size=min(n, len(ref_paths)), replace=False).tolist()\n",
    "    chosen = chosen[:min(n, len(chosen))]\n",
    "\n",
    "    plt.figure(figsize=(12, 4 * len(chosen)))\n",
    "\n",
    "    for r, p in enumerate(chosen):\n",
    "        if p not in model_path_to_idx:\n",
    "            # If this happens, student output paths do not match dataset paths.\n",
    "            continue\n",
    "        mi = model_path_to_idx[p]\n",
    "\n",
    "        image = Image.open(p).convert('RGB').resize((W, H))\n",
    "\n",
    "        amap_up = F.interpolate(\n",
    "            torch.tensor(maps[mi])[None, None].float(),\n",
    "            size=(H, W),\n",
    "            mode='bilinear',\n",
    "            align_corners=False,\n",
    "        ).squeeze().cpu().numpy()\n",
    "        amap_up = normalize_01(amap_up)\n",
    "\n",
    "        # GT contour if available\n",
    "        gt = None\n",
    "        y = 0\n",
    "        if p in ref_path_to_idx:\n",
    "            ri = ref_path_to_idx[p]\n",
    "            y = int(labels[ri]) if ri < len(labels) else 0\n",
    "            if y == 1 and masks is not None and ri < len(masks) and masks[ri] is not None:\n",
    "                gt = _mask_to_numpy(masks[ri])\n",
    "                if gt is not None and gt.shape != (H, W):\n",
    "                    gt = np.array(\n",
    "                        Image.fromarray((gt > 0.5).astype(np.uint8) * 255).resize((W, H), resample=Image.NEAREST)\n",
    "                    )\n",
    "                    gt = (gt > 127).astype(np.uint8)\n",
    "\n",
    "        # 1) Input\n",
    "        plt.subplot(len(chosen), 3, 3 * r + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        prefix = f'{title_prefix} | ' if title_prefix else ''\n",
    "        plt.title(f'{prefix}Input | y={y}')\n",
    "\n",
    "        # 2) Map\n",
    "        plt.subplot(len(chosen), 3, 3 * r + 2)\n",
    "        plt.imshow(amap_up)\n",
    "        plt.axis('off')\n",
    "        plt.title('Anomaly map (upsampled)')\n",
    "\n",
    "        # 3) Overlay\n",
    "        plt.subplot(len(chosen), 3, 3 * r + 3)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(amap_up, alpha=0.5)\n",
    "        if gt is not None:\n",
    "            plt.contour(gt, levels=[0.5])\n",
    "        plt.axis('off')\n",
    "        plt.title('Overlay (GT contour if available)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f1224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.415777Z",
     "start_time": "2025-12-21T11:59:20.413544Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelReport:\n",
    "    \"\"\"Summary tables + a small qualitative category shortlist for a model.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    df_cat: pd.DataFrame\n",
    "    df_summary: pd.DataFrame\n",
    "    qual_categories: List[str]\n",
    "\n",
    "\n",
    "def _safe_float(x) -> float:\n",
    "    \"\"\"Convert to float, returning NaN for None.\"\"\"\n",
    "    if x is None:\n",
    "        return float('nan')\n",
    "    return float(x)\n",
    "\n",
    "\n",
    "def _get_count_labels(labels: List[int]) -> Dict[str, int]:\n",
    "    \"\"\"Count good/anomaly labels in a list of 0/1 labels.\"\"\"\n",
    "    labels = list(labels)\n",
    "    n = len(labels)\n",
    "    n_anom = int(np.sum(np.array(labels) == 1))\n",
    "    n_good = n - n_anom\n",
    "    return {'n_test_total': n, 'n_test_good': n_good, 'n_test_anom': n_anom}\n",
    "\n",
    "\n",
    "def _mask_to_numpy(mask_obj) -> Optional[np.ndarray]:\n",
    "    \"\"\"Convert a stored mask (torch tensor or numpy array) to a 2D uint8 array.\"\"\"\n",
    "    if mask_obj is None:\n",
    "        return None\n",
    "    if hasattr(mask_obj, 'numpy'):\n",
    "        mask_obj = mask_obj.numpy()\n",
    "    m = np.asarray(mask_obj)\n",
    "    m = np.squeeze(m)\n",
    "    return m.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dab94c9a61e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.428964Z",
     "start_time": "2025-12-21T11:59:20.422188Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_outputs(\n",
    "    out_by_category: Dict[str, Dict[str, object]],\n",
    "    name: str,\n",
    "    *,\n",
    "    gt_labels_by_category: Optional[Dict[str, List[int]]] = None,\n",
    "    gt_masks_by_category: Optional[Dict[str, Optional[List[np.ndarray]]]] = None,\n",
    "    use_precomputed_image_metrics: bool = False,\n",
    "    use_precomputed_pixel_metrics: bool = False,\n",
    "    enforce_test_size_match: bool = False,\n",
    ") -> ModelReport:\n",
    "    \"\"\"Unify PaDiM + Student summarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_by_category:\n",
    "        Per-category outputs, each containing: 'paths', 'scores', 'score_maps', 'runtime_sec'.\n",
    "    name:\n",
    "        Display name for the report.\n",
    "    gt_labels_by_category:\n",
    "        Optional ground-truth image-level labels per category.\n",
    "        - If None, labels will be read from out_by_category[cat].get('labels', []).\n",
    "        - For student outputs, you typically pass labels from the PaDiM run.\n",
    "    gt_masks_by_category:\n",
    "        Optional ground-truth pixel masks per category (list of HxW uint8/bool arrays, or None).\n",
    "        Used only when pixel metrics are *computed* (not precomputed).\n",
    "    use_precomputed_image_metrics:\n",
    "        If True, reads out.get('image_metrics', {}) for AUROC/AP/F1/etc.\n",
    "        Otherwise computes image metrics from gt labels + out['scores'].\n",
    "    use_precomputed_pixel_metrics:\n",
    "        If True, reads out.get('pixel_auroc') and out.get('pixel_best_dice').\n",
    "        Otherwise, if gt masks are provided, computes pixel metrics from gt masks + out['score_maps'].\n",
    "    enforce_test_size_match:\n",
    "        If True, enforces that len(paths)==len(scores)==len(gt_labels) (helpful for student eval).\n",
    "        If False, only enforces len(paths)==len(scores).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ModelReport\n",
    "        Category table + summary table + a shortlist of categories for qualitative plots.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, object]] = []\n",
    "    all_labels: List[int] = []\n",
    "    all_scores: List[float] = []\n",
    "\n",
    "    # Decide which categories define the evaluation scope.\n",
    "    if gt_labels_by_category is not None:\n",
    "        categories = list(gt_labels_by_category.keys())\n",
    "    else:\n",
    "        categories = list(out_by_category.keys())\n",
    "\n",
    "    for category in categories:\n",
    "        out = out_by_category.get(category, {}) if isinstance(out_by_category, dict) else {}\n",
    "\n",
    "        for k in ['paths', 'scores', 'score_maps', 'runtime_sec']:\n",
    "            if k not in out:\n",
    "                raise ValueError(f'{name} output for category \"{category}\" is missing required key \"{k}\".')\n",
    "\n",
    "        # Resolve ground-truth labels.\n",
    "        if gt_labels_by_category is not None:\n",
    "            y_true = np.array(gt_labels_by_category.get(category, []), dtype=np.int64)\n",
    "        else:\n",
    "            y_true = np.array(out.get('labels', []), dtype=np.int64)\n",
    "\n",
    "        if enforce_test_size_match:\n",
    "            if len(out['paths']) != len(y_true):\n",
    "                raise ValueError(\n",
    "                    f'{name} output for category \"{category}\": paths length ({len(out[\"paths\"])}) must match '\n",
    "                    f'test set size ({len(y_true)}).'\n",
    "                )\n",
    "\n",
    "        if len(out['paths']) != len(out['scores']):\n",
    "            raise ValueError(f'{name} output for category \"{category}\": paths length does not match scores length.')\n",
    "\n",
    "        y_score = np.asarray(out['scores'], dtype=np.float32)\n",
    "\n",
    "        if enforce_test_size_match and len(y_score) != len(y_true):\n",
    "            raise ValueError(\n",
    "                f'{name} output for category \"{category}\": scores length ({len(y_score)}) must match labels length ({len(y_true)}).'\n",
    "            )\n",
    "\n",
    "        # Image metrics\n",
    "        if use_precomputed_image_metrics:\n",
    "            img_m = out.get('image_metrics', {}) or {}\n",
    "        else:\n",
    "            img_m = image_metrics(y_true, y_score) if len(y_true) else {}\n",
    "\n",
    "        # Pixel metrics\n",
    "        pixel_auc = float('nan')\n",
    "        pixel_dice = float('nan')\n",
    "\n",
    "        if use_precomputed_pixel_metrics:\n",
    "            pixel_auc = _safe_float(out.get('pixel_auroc', np.nan))\n",
    "            pixel_dice = _safe_float(out.get('pixel_best_dice', np.nan))\n",
    "        else:\n",
    "            masks = None\n",
    "            if gt_masks_by_category is not None:\n",
    "                masks = gt_masks_by_category.get(category, None)\n",
    "\n",
    "            # Only compute pixel metrics if we actually have GT masks and score maps\n",
    "            if masks is not None and len(masks):\n",
    "                sm = out.get('score_maps', None)\n",
    "                if sm is not None and len(sm):\n",
    "                    sm = np.asarray(sm)\n",
    "                    per_img_auc: List[float] = []\n",
    "                    per_img_dice: List[float] = []\n",
    "                    for i in range(min(len(masks), sm.shape[0])):\n",
    "                        gt = np.asarray(masks[i])\n",
    "                        if gt.ndim != 2:\n",
    "                            continue\n",
    "                        try:\n",
    "                            per_img_auc.append(compute_pixel_auc(gt, sm[i]))\n",
    "                            per_img_dice.append(compute_best_dice(gt, normalize_01(sm[i])))\n",
    "                        except Exception:\n",
    "                            # Bad shapes / NaNs should not crash summary; they just lose pixel credit.\n",
    "                            continue\n",
    "\n",
    "                    if len(per_img_auc) > 0:\n",
    "                        pixel_auc = float(np.nanmean(per_img_auc))\n",
    "                    if len(per_img_dice) > 0:\n",
    "                        pixel_dice = float(np.nanmean(per_img_dice))\n",
    "\n",
    "        counts = _get_count_labels(y_true.tolist())\n",
    "\n",
    "        rows.append({\n",
    "            'category': category,\n",
    "            **counts,\n",
    "            'auroc': _safe_float(img_m.get('auroc', np.nan)),\n",
    "            'ap': _safe_float(img_m.get('ap', np.nan)),\n",
    "            'best_f1': _safe_float(img_m.get('best_f1', np.nan)),\n",
    "            'best_thr': _safe_float(img_m.get('best_thr', np.nan)),\n",
    "            'bal_acc': _safe_float(img_m.get('bal_acc', np.nan)),\n",
    "            'pixel_auroc': _safe_float(pixel_auc),\n",
    "            'pixel_best_dice': _safe_float(pixel_dice),\n",
    "            'runtime_sec': _safe_float(float(out['runtime_sec'])),\n",
    "        })\n",
    "\n",
    "        all_labels.extend(y_true.tolist())\n",
    "        all_scores.extend(y_score.tolist())\n",
    "\n",
    "    df_cat = pd.DataFrame(rows).sort_values('auroc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    overall_micro = (\n",
    "        image_metrics(np.array(all_labels, dtype=np.int64), np.array(all_scores, dtype=np.float32))\n",
    "        if len(all_labels) else {}\n",
    "    )\n",
    "    macro_cols = ['auroc', 'ap', 'best_f1', 'bal_acc', 'pixel_auroc', 'pixel_best_dice', 'runtime_sec']\n",
    "    overall_macro = df_cat[macro_cols].mean(numeric_only=True).to_dict() if len(df_cat) else {}\n",
    "\n",
    "    df_summary = pd.DataFrame([\n",
    "        {'scope': 'macro_mean_across_categories', **overall_macro},\n",
    "        {'scope': 'micro_all_test_images', **{\n",
    "            'auroc': _safe_float(overall_micro.get('auroc', np.nan)),\n",
    "            'ap': _safe_float(overall_micro.get('ap', np.nan)),\n",
    "            'best_f1': _safe_float(overall_micro.get('best_f1', np.nan)),\n",
    "            'bal_acc': _safe_float(overall_micro.get('bal_acc', np.nan)),\n",
    "            # There is no meaningful 'micro' pixel score without merging all pixels;\n",
    "            # keep reporting the macro means for pixel metrics.\n",
    "            'pixel_auroc': _safe_float(overall_macro.get('pixel_auroc', np.nan)),\n",
    "            'pixel_best_dice': _safe_float(overall_macro.get('pixel_best_dice', np.nan)),\n",
    "            'runtime_sec': _safe_float(overall_macro.get('runtime_sec', np.nan)),\n",
    "        }},\n",
    "    ])\n",
    "\n",
    "    # Qualitative shortlist: best / mid / worst (or all if <3 cats)\n",
    "    if len(df_cat) >= 3:\n",
    "        qual_categories = [\n",
    "            str(df_cat.iloc[0]['category']),\n",
    "            str(df_cat.iloc[len(df_cat) // 2]['category']),\n",
    "            str(df_cat.iloc[-1]['category']),\n",
    "        ]\n",
    "    else:\n",
    "        qual_categories = df_cat['category'].astype(str).tolist()\n",
    "\n",
    "    return ModelReport(name=name, df_cat=df_cat, df_summary=df_summary, qual_categories=qual_categories)\n",
    "\n",
    "\n",
    "def summarize_padim_outputs(padim_out_by_category: Dict[str, Dict[str, object]], name: str = 'PaDiM') -> ModelReport:\n",
    "    \"\"\"Backwards-compatible wrapper around summarize_outputs().\"\"\"\n",
    "    return summarize_outputs(\n",
    "        padim_out_by_category,\n",
    "        name=name,\n",
    "        gt_labels_by_category=None,  # read from padim outputs\n",
    "        gt_masks_by_category=None,\n",
    "        use_precomputed_image_metrics=True,\n",
    "        use_precomputed_pixel_metrics=True,\n",
    "        enforce_test_size_match=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_student_outputs(\n",
    "    student_out_by_cat: Dict[str, Dict[str, object]],\n",
    "    padim_out_by_category: Dict[str, Dict[str, object]],\n",
    "    name: str = 'Student',\n",
    ") -> ModelReport:\n",
    "    \"\"\"Backwards-compatible wrapper around summarize_outputs().\n",
    "\n",
    "    Uses PaDiM's labels/masks as ground truth, and computes metrics from student scores/score_maps.\n",
    "    \"\"\"\n",
    "    gt_labels_by_category = {cat: (pad_out.get('labels', []) or []) for cat, pad_out in padim_out_by_category.items()}\n",
    "    gt_masks_by_category = {cat: pad_out.get('masks', None) for cat, pad_out in padim_out_by_category.items()}\n",
    "\n",
    "    return summarize_outputs(\n",
    "        student_out_by_cat,\n",
    "        name=name,\n",
    "        gt_labels_by_category=gt_labels_by_category,\n",
    "        gt_masks_by_category=gt_masks_by_category,\n",
    "        use_precomputed_image_metrics=False,\n",
    "        use_precomputed_pixel_metrics=False,\n",
    "        enforce_test_size_match=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25134863bebcbf3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.433912Z",
     "start_time": "2025-12-21T11:59:20.431575Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_model_tables(report: ModelReport) -> None:\n",
    "    \"\"\"Display the per-category and overall summary tables for one model.\"\"\"\n",
    "    display(report.df_cat)\n",
    "    display(report.df_summary)\n",
    "    print('Qualitative categories (best/middle/worst):', report.qual_categories)\n",
    "\n",
    "\n",
    "def _plot_barh_metric(df: pd.DataFrame, metric: str, title: str, xlabel: str) -> None:\n",
    "    \"\"\"Horizontal bar plot of a single metric across categories.\"\"\"\n",
    "    d = df[['category', metric]].copy()\n",
    "    d[metric] = pd.to_numeric(d[metric], errors='coerce')\n",
    "    d = d.dropna(subset=[metric])\n",
    "\n",
    "    if len(d) == 0:\n",
    "        print(f'[{title}] nothing to plot (all NaN).')\n",
    "        return\n",
    "\n",
    "    d = d.sort_values(metric, ascending=True).reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(10, max(4, 0.3 * len(d))))\n",
    "    plt.barh(d['category'], d[metric])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_charts(report: ModelReport, title_prefix: Optional[str] = None) -> None:\n",
    "    \"\"\"Draw a small set of standard charts from a `ModelReport`.\"\"\"\n",
    "    tp = title_prefix or report.name\n",
    "    df = report.df_cat\n",
    "\n",
    "    _plot_barh_metric(df, 'auroc', f'{tp}: Image-level AUROC per category', 'Image AUROC')\n",
    "    _plot_barh_metric(df, 'ap', f'{tp}: Image-level AP per category', 'Average Precision')\n",
    "    _plot_barh_metric(df, 'best_f1', f'{tp}: Best F1 per category', 'Best F1')\n",
    "\n",
    "    _plot_barh_metric(df, 'pixel_auroc', f'{tp}: Pixel-level AUROC per category', 'Pixel AUROC')\n",
    "    _plot_barh_metric(df, 'runtime_sec', f'{tp}: Runtime per category', 'Runtime (sec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf093688f1d358b",
   "metadata": {},
   "source": [
    "## Qualitative visualization utilities\n",
    "\n",
    "We use **fixed samples** per category (selected once near the top) to keep qualitative comparisons consistent:\n",
    "\n",
    "Per sample, we show:\n",
    "1. Input image\n",
    "2. Anomaly map (upsampled)\n",
    "3. Overlay + GT contour (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d1c7a75331d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T11:59:20.438776Z",
     "start_time": "2025-12-21T11:59:20.436135Z"
    }
   },
   "outputs": [],
   "source": [
    "def _match_sample_indices(\n",
    "    all_paths: List[str],\n",
    "    sample_paths: List[str],\n",
    "    *,\n",
    "    seed: int = 0,\n",
    "    n: int = 6,\n",
    ") -> List[int]:\n",
    "    \"\"\"Pick indices for qualitative visualization.\n",
    "\n",
    "    We prefer the fixed `sample_paths` (exact string match against `all_paths`).\n",
    "    If none match (e.g., paths were saved differently), we fall back to `n` random indices.\n",
    "    \"\"\"\n",
    "    path_to_idx = {p: i for i, p in enumerate(all_paths)}\n",
    "    idxs = [path_to_idx[str(p)] for p in sample_paths if str(p) in path_to_idx]\n",
    "\n",
    "    if len(idxs) == 0:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        n_eff = min(n, len(all_paths))\n",
    "        return list(np.sort(rng.choice(len(all_paths), size=n_eff, replace=False)))\n",
    "\n",
    "    return idxs[:min(n, len(idxs))]\n",
    "\n",
    "def plot_model_qualitative(\n",
    "    report: ModelReport,\n",
    "    out_by_cat: Dict[str, Dict[str, object]],\n",
    "    sample_paths_by_cat: Dict[str, List[str]],\n",
    "    mode: str = 'padim',\n",
    "    gt_out_by_cat: Optional[Dict[str, Dict[str, object]]] = None,\n",
    "    categories: Optional[List[str]] = None,\n",
    "    n_per_cat: int = 6,\n",
    "    seed: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Qualitative visualization wrapper used after running each model.\"\"\"\n",
    "    global CATEGORIES\n",
    "\n",
    "    for category in CATEGORIES:\n",
    "        if category not in sample_paths_by_cat:\n",
    "            continue\n",
    "        sample_paths = sample_paths_by_cat[category][:n_per_cat]\n",
    "\n",
    "        print('=' * 80)\n",
    "        print(f'{report.name} | category: {category}')\n",
    "\n",
    "        if mode == 'padim':\n",
    "            visualize_examples(out_by_cat[category], gt_out=None, sample_paths=sample_paths, n=len(sample_paths), seed=seed, title_prefix=report.name)\n",
    "        elif mode == 'student':\n",
    "            assert gt_out_by_cat is not None, 'gt_out_by_cat is required for student qualitative.'\n",
    "            visualize_examples(out_by_cat[category], gt_out=gt_out_by_cat[category], sample_paths=sample_paths, n=len(sample_paths), seed=seed, title_prefix=report.name)\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: ' + mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e3cfb8673894e",
   "metadata": {},
   "source": [
    "## 8) Run models\n",
    "\n",
    "This section:\n",
    "- runs PaDiM and student's model per category\n",
    "- stores results\n",
    "- calls the reporting functions to show:\n",
    "  - tables (per-category + overall)\n",
    "  - plots\n",
    "  - qualitative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16058a4f0b8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:00:59.722120Z",
     "start_time": "2025-12-21T11:59:20.441326Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(CATEGORIES) > 0, 'No categories found. Set DATA_ROOT / DATASET_ROOT correctly.'\n",
    "\n",
    "PADIM_DEFAULTS = dict(\n",
    "    backbone='resnet18',\n",
    "    pretrained=True,  # set False if your environment cannot download weights\n",
    "    layers=('layer1', 'layer2', 'layer3'),\n",
    "    d=100,\n",
    "    smooth_sigma=2.0,  # set None to disable\n",
    "    image_aggregation='mean_topk',\n",
    "    topk_fraction=0.1,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "padim_out_by_category = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    config = PadimConfig(category=category, **PADIM_DEFAULTS)\n",
    "    out = run_padim_category(config)\n",
    "    padim_out_by_category[category] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d94b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:04.971618Z",
     "start_time": "2025-12-21T12:00:59.739404Z"
    }
   },
   "outputs": [],
   "source": [
    "padim_report = summarize_padim_outputs(padim_out_by_category, name='PaDiM')\n",
    "show_model_tables(padim_report)\n",
    "plot_model_charts(padim_report)\n",
    "\n",
    "plot_model_qualitative(\n",
    "    padim_report,\n",
    "    padim_out_by_category,\n",
    "    QUAL_SAMPLE_PATHS_BY_CAT,\n",
    "    mode='padim',\n",
    "    categories=padim_report.qual_categories,\n",
    "    n_per_cat=6,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424670e5842187ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:04.997738Z",
     "start_time": "2025-12-21T12:01:04.995618Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you haven't implemented the student method yet, this notebook can still run end-to-end.\n",
    "# We create a dummy `student_out_by_cat` that:\n",
    "# - uses the same `paths` as PaDiM\n",
    "# - perturbs PaDiM `score_maps`\n",
    "# - computes `scores` from the perturbed maps\n",
    "# - sets a plausible `runtime_sec`\n",
    "\n",
    "def make_dummy_student_out_by_cat(\n",
    "    padim_out_by_category: Dict[str, Dict[str, object]],\n",
    "    *,\n",
    "    seed: int = 0,\n",
    "    noise: float = 0.15,\n",
    ") -> Dict[str, Dict[str, object]]:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    out: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    for cat, p in padim_out_by_category.items():\n",
    "        paths = list(p['paths'])\n",
    "        pmaps = np.asarray(p['score_maps'], dtype=np.float32)\n",
    "\n",
    "        smaps = pmaps + noise * rng.randn(*pmaps.shape).astype(np.float32)\n",
    "        smaps = np.stack([normalize_01(m) for m in smaps], axis=0).astype(np.float32)\n",
    "\n",
    "        scores = np.asarray([aggregate_image_score(m) for m in smaps], dtype=np.float32)\n",
    "        scores = normalize_01(scores)\n",
    "\n",
    "        runtime_sec = float(p.get('runtime_sec', 0.0)) * 0.8 + float(rng.rand() * 0.02)\n",
    "\n",
    "        out[cat] = {\n",
    "            'paths': paths,\n",
    "            'scores': scores.tolist(),\n",
    "            'score_maps': smaps,\n",
    "            'runtime_sec': runtime_sec,\n",
    "        }\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310d75856c2f325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:05.031789Z",
     "start_time": "2025-12-21T12:01:05.000408Z"
    }
   },
   "outputs": [],
   "source": [
    "if student_out_by_cat is None:\n",
    "    student_out_by_cat = make_dummy_student_out_by_cat(padim_out_by_category, seed=0, noise=0.15)\n",
    "    print('Generated dummy student_out_by_cat (so the notebook can run without a student implementation).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4e8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:05.043288Z",
     "start_time": "2025-12-21T12:01:05.042025Z"
    }
   },
   "outputs": [],
   "source": [
    "# After implementing `StudentMethod`, run it per category and store outputs here:\n",
    "#   student_out_by_cat[category] = student.predict(test_loader)\n",
    "#\n",
    "# The `predict` output must align with the test loader ordering (paths/scores/score_maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc6dee8beed81e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:10.442565Z",
     "start_time": "2025-12-21T12:01:05.046122Z"
    }
   },
   "outputs": [],
   "source": [
    "student_report = summarize_student_outputs(student_out_by_cat, padim_out_by_category, name='Student')\n",
    "show_model_tables(student_report)\n",
    "plot_model_charts(student_report)\n",
    "\n",
    "# Qualitative: same fixed samples as PaDiM\n",
    "plot_model_qualitative(\n",
    "    student_report,\n",
    "    student_out_by_cat,\n",
    "    QUAL_SAMPLE_PATHS_BY_CAT,\n",
    "    mode='student',\n",
    "    gt_out_by_cat=padim_out_by_category,  # for GT contours/labels\n",
    "    categories=student_report.qual_categories,\n",
    "    n_per_cat=6,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6045264d4fc5d66",
   "metadata": {},
   "source": [
    "## 9) PaDiM vs Student comparison (4 points, just running cells, no TODOs)\n",
    "\n",
    "We compare PaDiM vs the student method:\n",
    "- per-category metrics (side-by-side + deltas)\n",
    "- overall micro/macro summary (side-by-side + deltas)\n",
    "- integrated qualitative comparison on the **same fixed samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3aed371dee902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:10.478340Z",
     "start_time": "2025-12-21T12:01:10.473968Z"
    }
   },
   "outputs": [],
   "source": [
    "COMPARISON_METRICS = [\n",
    "    'auroc',\n",
    "    'ap',\n",
    "    'best_f1',\n",
    "    'bal_acc',\n",
    "    'pixel_auroc',\n",
    "    'pixel_best_dice',\n",
    "    'runtime_sec',\n",
    "]\n",
    "\n",
    "\n",
    "def build_category_comparison_df(\n",
    "    padim_report: ModelReport,\n",
    "    student_report: ModelReport,\n",
    "    metrics: List[str] = COMPARISON_METRICS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge per-category tables and add delta columns (student - padim).\"\"\"\n",
    "    p = padim_report.df_cat.copy()\n",
    "    s = student_report.df_cat.copy()\n",
    "\n",
    "    keep_cols = ['category'] + [m for m in metrics if m in p.columns]\n",
    "    p = p[keep_cols].rename(columns={m: f'padim_{m}' for m in keep_cols if m != 'category'})\n",
    "\n",
    "    keep_cols_s = ['category'] + [m for m in metrics if m in s.columns]\n",
    "    s = s[keep_cols_s].rename(columns={m: f'student_{m}' for m in keep_cols_s if m != 'category'})\n",
    "\n",
    "    df = p.merge(s, on='category', how='inner')\n",
    "\n",
    "    for m in metrics:\n",
    "        pm = f'padim_{m}'\n",
    "        sm = f'student_{m}'\n",
    "        if pm in df.columns and sm in df.columns:\n",
    "            df[f'delta_{m}'] = pd.to_numeric(df[sm], errors='coerce') - pd.to_numeric(df[pm], errors='coerce')\n",
    "\n",
    "    return df.sort_values('delta_auroc', ascending=False) if 'delta_auroc' in df.columns else df\n",
    "\n",
    "\n",
    "def build_overall_comparison_df(padim_report: ModelReport, student_report: ModelReport) -> pd.DataFrame:\n",
    "    \"\"\"Build a 2-row table comparing macro and micro summaries.\"\"\"\n",
    "    def _row(df: pd.DataFrame, scope: str) -> pd.Series:\n",
    "        r = df[df['scope'] == scope].iloc[0]\n",
    "        return r\n",
    "\n",
    "    scopes = ['macro_mean_across_categories', 'micro_all_test_images']\n",
    "    rows = []\n",
    "\n",
    "    for scope in scopes:\n",
    "        pr = _row(padim_report.df_summary, scope)\n",
    "        sr = _row(student_report.df_summary, scope)\n",
    "\n",
    "        row = {'scope': scope}\n",
    "        for m in COMPARISON_METRICS:\n",
    "            if m in pr:\n",
    "                row[f'padim_{m}'] = pr[m]\n",
    "            if m in sr:\n",
    "                row[f'student_{m}'] = sr[m]\n",
    "            if f'padim_{m}' in row and f'student_{m}' in row:\n",
    "                row[f'delta_{m}'] = _safe_float(row[f'student_{m}']) - _safe_float(row[f'padim_{m}'])\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_delta_bar(df_cat_cmp: pd.DataFrame, metric: str, title: Optional[str] = None) -> None:\n",
    "    \"\"\"Plot per-category delta bars for one metric (student - padim).\"\"\"\n",
    "    col = f'delta_{metric}'\n",
    "    if col not in df_cat_cmp.columns:\n",
    "        print(f'No column {col} in comparison table.')\n",
    "        return\n",
    "\n",
    "    d = df_cat_cmp[['category', col]].copy()\n",
    "    d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "    d = d.dropna(subset=[col]).sort_values(col, ascending=True)\n",
    "\n",
    "    if len(d) == 0:\n",
    "        print(f'[{metric}] nothing to plot.')\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, max(4, 0.3 * len(d))))\n",
    "    plt.barh(d['category'], d[col])\n",
    "    plt.xlabel(f'Delta {metric} (student - padim)')\n",
    "    plt.title(title or f'Delta {metric} per category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5ead6b49b585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:10.652615Z",
     "start_time": "2025-12-21T12:01:10.481113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quantitative comparison: PaDiM vs Student\n",
    "\n",
    "assert 'padim_report' in globals(), 'Run the PaDiM reporting cell first (padim_report).'\n",
    "assert 'student_report' in globals(), 'Run the student reporting cell first (student_report).'\n",
    "\n",
    "df_cat_cmp = build_category_comparison_df(padim_report, student_report)\n",
    "df_overall_cmp = build_overall_comparison_df(padim_report, student_report)\n",
    "\n",
    "print('Per-category comparison (student - padim deltas included):')\n",
    "display(df_cat_cmp)\n",
    "\n",
    "print('Overall comparison (macro + micro):')\n",
    "display(df_overall_cmp)\n",
    "\n",
    "# A few standard delta plots\n",
    "plot_delta_bar(df_cat_cmp, 'auroc', 'Delta Image AUROC (student - padim)')\n",
    "plot_delta_bar(df_cat_cmp, 'ap', 'Delta Image AP (student - padim)')\n",
    "plot_delta_bar(df_cat_cmp, 'pixel_auroc', 'Delta Pixel AUROC (student - padim)')\n",
    "plot_delta_bar(df_cat_cmp, 'runtime_sec', 'Delta Runtime (student - padim)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7735812bd1a3d",
   "metadata": {},
   "source": [
    "### Integrated qualitative comparison (same fixed samples)\n",
    "\n",
    "For the same images, show:\n",
    "\n",
    "**Input | PaDiM overlay | Student overlay**\n",
    "\n",
    "This makes it easy to see whether the student method localizes defects similarly (or better).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0d2da494f0cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:10.716046Z",
     "start_time": "2025-12-21T12:01:10.709316Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_padim_vs_student_qualitative(\n",
    "    padim_report: ModelReport,\n",
    "    student_report: ModelReport,\n",
    "    padim_out_by_cat: Dict[str, Dict[str, object]],\n",
    "    student_out_by_cat: Dict[str, Dict[str, object]],\n",
    "    fixed_samples_by_cat: Dict[str, List[str]],\n",
    "    *,\n",
    "    categories: Optional[List[str]] = None,\n",
    "    n_per_cat: int = 6,\n",
    "    seed: int = 0,\n",
    "    vis_size: tuple[int, int] = (224, 224),\n",
    "):\n",
    "    \"\"\"Integrated qualitative comparison on the SAME fixed samples.\n",
    "\n",
    "    For each category, shows rows of fixed samples with 3 columns:\n",
    "      1) Input image\n",
    "      2) PaDiM anomaly map overlay (+ GT contour if available)\n",
    "      3) Student anomaly map overlay (+ GT contour if available)\n",
    "\n",
    "    Notes:\n",
    "      - Assumes student_out_by_cat[cat]['score_maps'] exists (required).\n",
    "      - Uses PaDiM masks/labels for GT contours and y=0/1.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    H, W = vis_size\n",
    "\n",
    "    def _mask_to_np_local(mask) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert a mask object to a float32 numpy array.\"\"\"\n",
    "        if mask is None:\n",
    "            return None\n",
    "        if torch.is_tensor(mask):\n",
    "            m = mask.detach().cpu().numpy()\n",
    "        else:\n",
    "            m = np.asarray(mask)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "    def _upsample_map_to_vis(map2d: np.ndarray, size_hw: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Bilinear upsample for anomaly maps.\"\"\"\n",
    "        th = torch.tensor(map2d, dtype=torch.float32)[None, None]\n",
    "        up = F.interpolate(th, size=size_hw, mode='bilinear', align_corners=False)\n",
    "        return up.squeeze(0).squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    def _upsample_mask_to_vis(mask2d: np.ndarray, size_hw: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Nearest-neighbor upsample for binary masks.\"\"\"\n",
    "        th = torch.tensor(mask2d, dtype=torch.float32)[None, None]\n",
    "        up = F.interpolate(th, size=size_hw, mode='nearest')\n",
    "        up = up.squeeze(0).squeeze(0).cpu().numpy().astype(np.float32)\n",
    "        return (up > 0.5).astype(np.float32)\n",
    "\n",
    "    # Show all categories by default\n",
    "    cats = categories or list(fixed_samples_by_cat.keys()) or sorted(list(padim_out_by_cat.keys()))\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    for cat in cats:\n",
    "        if cat not in padim_out_by_cat or cat not in student_out_by_cat:\n",
    "            continue\n",
    "\n",
    "        pad = padim_out_by_cat[cat]\n",
    "        stu = student_out_by_cat[cat]\n",
    "\n",
    "        paths = [str(p) for p in pad.get('paths', [])]\n",
    "        labels = list(pad.get('labels', []))\n",
    "        masks = pad.get('masks', None)\n",
    "\n",
    "        pad_maps = pad.get('score_maps', None)\n",
    "        stu_maps = stu.get('score_maps', None)\n",
    "\n",
    "        if stu_maps is None:\n",
    "            raise ValueError(f\"Student output for category '{cat}' is missing 'score_maps'.\")\n",
    "\n",
    "        # Use fixed samples when available; otherwise pick random indices.\n",
    "        fixed = fixed_samples_by_cat.get(cat, [])\n",
    "        fixed = [str(p) for p in fixed]\n",
    "        idxs: List[int] = []\n",
    "\n",
    "        if len(fixed) > 0:\n",
    "            # Map fixed sample paths to indices in `paths` using suffix under /test/\n",
    "            def _suffix_under_test(p: str) -> str:\n",
    "                parts = Path(p).as_posix().split('/test/')\n",
    "                return parts[-1] if len(parts) > 1 else Path(p).name\n",
    "\n",
    "            idx_by_suffix = {_suffix_under_test(p): i for i, p in enumerate(paths)}\n",
    "            for fp in fixed:\n",
    "                key = _suffix_under_test(fp)\n",
    "                if key in idx_by_suffix:\n",
    "                    idxs.append(idx_by_suffix[key])\n",
    "        else:\n",
    "            idxs = list(range(len(paths)))\n",
    "            rng.shuffle(idxs)\n",
    "\n",
    "        idxs = idxs[:min(n_per_cat, len(idxs))]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(14, 4 * len(idxs)))\n",
    "        for r, i in enumerate(idxs):\n",
    "            img = Image.open(paths[i]).convert('RGB').resize((W, H))\n",
    "            y = int(labels[i]) if i < len(labels) else 0\n",
    "\n",
    "            # Prepare GT contour (only for anomalies)\n",
    "            gt = None\n",
    "            if y == 1 and masks is not None and i < len(masks) and masks[i] is not None:\n",
    "                gt_np = _mask_to_np_local(masks[i])\n",
    "                if gt_np is not None:\n",
    "                    gt_np = gt_np.squeeze()\n",
    "                    if gt_np.shape != (H, W):\n",
    "                        gt_np = _upsample_mask_to_vis(gt_np, (H, W))\n",
    "                    else:\n",
    "                        gt_np = (gt_np > 0.5).astype(np.float32)\n",
    "                    gt = gt_np\n",
    "\n",
    "            def _get_map(maps_obj, idx: int) -> Optional[np.ndarray]:\n",
    "                \"\"\"Get a normalized HxW map for index idx.\"\"\"\n",
    "                if maps_obj is None or idx >= len(maps_obj):\n",
    "                    return None\n",
    "                m = np.asarray(maps_obj[idx]).astype(np.float32)\n",
    "                if m.ndim != 2:\n",
    "                    m = m.squeeze()\n",
    "                if m.shape != (H, W):\n",
    "                    m = _upsample_map_to_vis(m, (H, W))\n",
    "                return normalize_01(m)\n",
    "\n",
    "            pad_map = _get_map(pad_maps, i)\n",
    "            stu_map = _get_map(stu_maps, i)\n",
    "\n",
    "            # 1) Input\n",
    "            plt.subplot(len(idxs), 3, 3 * r + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Input | y={y}')\n",
    "\n",
    "            # 2) PaDiM overlay\n",
    "            plt.subplot(len(idxs), 3, 3 * r + 2)\n",
    "            plt.imshow(img)\n",
    "            if pad_map is not None:\n",
    "                plt.imshow(pad_map, alpha=0.5)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No PaDiM score_maps', ha='center', va='center')\n",
    "            if gt is not None:\n",
    "                plt.contour(gt, levels=[0.5])\n",
    "            plt.axis('off')\n",
    "            plt.title('PaDiM overlay')\n",
    "\n",
    "            # 3) Student overlay\n",
    "            plt.subplot(len(idxs), 3, 3 * r + 3)\n",
    "            plt.imshow(img)\n",
    "            if stu_map is not None:\n",
    "                plt.imshow(stu_map, alpha=0.5)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No student score_maps', ha='center', va='center')\n",
    "            if gt is not None:\n",
    "                plt.contour(gt, levels=[0.5])\n",
    "            plt.axis('off')\n",
    "            plt.title('Student overlay')\n",
    "\n",
    "        plt.suptitle(f'Qualitative comparison: {cat}', y=1.02, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581de44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:15.961514Z",
     "start_time": "2025-12-21T12:01:10.725319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Integrated qualitative comparison (Input | PaDiM | Student) on the SAME fixed samples\n",
    "if 'padim_report' in globals() and 'student_report' in globals() and isinstance(student_out_by_cat, dict) and len(\n",
    "        student_out_by_cat):\n",
    "    plot_padim_vs_student_qualitative(\n",
    "        padim_report,\n",
    "        student_report,\n",
    "        padim_out_by_category,\n",
    "        student_out_by_cat,\n",
    "        QUAL_SAMPLE_PATHS_BY_CAT,\n",
    "        categories=CATEGORIES,\n",
    "        n_per_cat=6,\n",
    "        seed=SEED,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        'Run PaDiM + Student reports first (padim_report, student_report), and ensure student_out_by_cat is populated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c43786da9d56f0",
   "metadata": {},
   "source": [
    "## 10) Competitive leaderboard metrics\n",
    "\n",
    "Because humans love turning nuanced work into a single number, here are **three comparable scores** (all in **[0, 1]**) designed for a competitive leaderboard.\n",
    "\n",
    "All scores are computed from the **macro mean across categories** (so large categories do not dominate small ones).\n",
    "\n",
    "### Definitions\n",
    "\n",
    "1. **ImageScore**  \n",
    "   `ImageScore = mean(Image AUROC, Image AP)`\n",
    "\n",
    "2. **PixelScore**  \n",
    "   `PixelScore = mean(Pixel AUROC, Pixel Best Dice)`  \n",
    "   (If pixel metrics are unavailable, this becomes NaN.)\n",
    "\n",
    "3. **OverallScore**  \n",
    "   `OverallScore = mean(ImageScore, PixelScore)`  \n",
    "   (Uses `nanmean`, so if PixelScore is NaN you effectively get ImageScore.)\n",
    "\n",
    "### Extra credit points (3 teams)\n",
    "\n",
    "| Rank | ImageScore bonus | PixelScore bonus | OverallScore bonus |\n",
    "|------|-----------------:|-----------------:|-------------------:|\n",
    "| 1st  |                4 |                5 |                  6 |\n",
    "| 2nd  |                2 |                3 |                  4 |\n",
    "| 3rd  |                0 |                1 |                  2 |\n",
    "\n",
    "- **Total extra points** for a team is the **sum** of its bonuses across the three metrics:\n",
    "  `extra_points = ImageScore_bonus + PixelScore_bonus + OverallScore_bonus`\n",
    "  (Maximum extra = **15** points.)\n",
    "\n",
    "To be ranked for each metric, your implementation must produce **valid, non-NaN values** for that metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9062ce49e31c5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:15.999560Z",
     "start_time": "2025-12-21T12:01:15.997302Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_leaderboard_scores(report: ModelReport, model_name: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute compact leaderboard scores from a ModelReport.\n",
    "\n",
    "    Uses the macro mean across categories from report.df_summary.\n",
    "    Returns scores in [0, 1] when underlying metrics are defined that way.\n",
    "    \"\"\"\n",
    "    df = report.df_summary\n",
    "    macro = df.loc[df['scope'] == 'macro_mean_across_categories']\n",
    "    if len(macro) == 0:\n",
    "        raise ValueError(f'No macro summary row found in report.df_summary for model={model_name!r}.')\n",
    "    macro = macro.iloc[0]\n",
    "\n",
    "    # Image-level score: combine threshold-free AUROC with PR-focused AP\n",
    "    image_score = float(np.nanmean([macro.get('auroc', np.nan), macro.get('ap', np.nan)]))\n",
    "\n",
    "    # Pixel-level score: combine AUROC with overlap-based Dice\n",
    "    pixel_score = float(np.nanmean([macro.get('pixel_auroc', np.nan), macro.get('pixel_best_dice', np.nan)]))\n",
    "\n",
    "    # Overall score: combines image and pixel quality (nan-safe)\n",
    "    overall_score = float(np.nanmean([image_score, pixel_score]))\n",
    "\n",
    "    runtime_sec = float(macro.get('runtime_sec', np.nan))\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'ImageScore': image_score,\n",
    "        'PixelScore': pixel_score,\n",
    "        'OverallScore': overall_score,\n",
    "        'runtime_sec': runtime_sec,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf4aa879b5b6c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:01:16.006289Z",
     "start_time": "2025-12-21T12:01:16.002288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: compare PaDiM vs Student on this run\n",
    "rows = []\n",
    "if 'padim_report' in globals():\n",
    "    rows.append(compute_leaderboard_scores(padim_report, 'PaDiM'))\n",
    "if 'student_report' in globals():\n",
    "    rows.append(compute_leaderboard_scores(student_report, 'Student'))\n",
    "\n",
    "if len(rows) > 0:\n",
    "    leaderboard_df = pd.DataFrame(rows).sort_values(\n",
    "        by=['OverallScore', 'runtime_sec'],\n",
    "        ascending=[False, True],\n",
    "        kind='mergesort',\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    display(leaderboard_df)\n",
    "else:\n",
    "    print('No reports found (expected padim_report and/or student_report). Run the evaluation cells above first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
